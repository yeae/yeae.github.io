---
title: Material_E
date: 2022-10-04 15:13:33
tags:
---
<details> <summary>A Squeeze on the Global Middle Class</summary>

[Bloomberg Businessweek|April 12, 2021](https://www.magzter.com/stories/Business/Bloomberg-Businessweek/A-Squeeze-on-the-Global-Middle-Class)
An estimated 150 million people slipped down the economic ladder in 2020, the first setback in almost three decades
By Cristina Lindblad
One of the most economically significant trends of the past few decades has been the emergence of a global middle class. __The expectation that this cohort of consumers would continue to grow relentlessly, as rising incomes in developing countries lifted millions out of poverty each year, has been a central assumption in multinationals’ business plans and the portfolio strategies of professional investors.__

You can now add that to the list of economic truths that have been upended by this pandemic. For the first time since the 1990s, the global middle class shrank last year, according to Pew Research Center estimates. About 150 million people tumbled down the economic ladder in 2020, with South Asia and sub-Saharan Africa seeing the biggest declines.

Defining the parameters of this global middle class has long been a contentious exercise. Pew, which has been researching the topic for more than a decade, labels as middle income those making from $10.01 to $20 a day, using data that smooth out differences in purchasing power across countries. In Pew’s analysis, there’s a separate upper-middle-income band made up of those earning $20.01 to $50 a day. (Note that $50 per day falls shy of what a minimum wage worker in the U.S. takes home pretax for an eight-hour day.) Others have opted for a more expansive $10 to $100 a day definition.

Taken together, Pew’s middle-income and upper-middle-income brackets encompass roughly 2.5 billion people—or a third of the world’s population. Buried inside these big numbers are many personal stories. Here we bring you four, from South Africa, Thailand, India, and Brazil. They’re tales of hard-won successes that evaporated overnight, along with well-paying jobs. Of once-accessible luxuries, like steak for dinner or home internet access, now out of reach.

Strivers face a far more uncertain future than in years past. In its latest World Economic Outlook, released in full on April 6, the International Monetary Fund predicts the global economy in 2024 will be 3% smaller than it would have been without the pandemic, largely because developing world governments have less room to spend their way to recovery, as the U.S. and Europe are doing.

The divergences are stark. India will end 2021 with a gross domestic product that’s 5.2% smaller than it would have been otherwise, according to forecasts by Bloomberg Economics. Indonesia’s output will be 9.2% smaller than its pre-crisis trend foretold. The U.S.? Just 1.6% smaller.

The global economy is “bifurcating,” says Carmen Reinhart, the World Bank’s chief economist, who cautions that a rebound in growth rates should not be mistaken for a lasting recovery. One big issue: Immunizations are proceeding far more slowly in poorer countries that have yet to gain the same access to vaccines as the rich world has.

But it goes further than that. Reinhart is concerned that in some countries governments may be forced to switch into austerity mode prematurely because they can’t shoulder their expanded debt loads. And while inflation is muted in the U.S. and Europe, in places such as Brazil food prices are soaring, leading central banks to tighten monetary policy prematurely. “This has been a very long year, and I think the damage has been underestimated,” she says. —Shawn Donnan</details>


<details> <summary>The Nature of Genius </summary>

[raw](https://ieltsmaterial.com/the-nature-of-genius-answers/)

There has always been an interest in geniuses and prodigies. The word ‘genius’, from the Latin gens (= family) and the term ‘genius’, meaning ‘begetter’, comes from the early Roman cult of a divinity as the head of the family. In its earliest form, genius was concerned with the ability of the head of the family, the paterfamilias, to perpetuate himself. Gradually, genius came to represent a person’s characteristics and thence an individual’s highest attributes derived from his ‘genius’ or guiding spirit. Today, people still look to stars or genes, astrology or genetics, in the hope of finding the source of exceptional abilities or personal characteristics.

The concept of genius and of gifts has become part of our folk culture, and attitudes are ambivalent towards them. We envy the gifted and mistrust them. In the mythology of giftedness, it is popularly believed that if people are talented in one area, they must be defective in another, that intellectuals are impractical, that prodigies burn too brightly too soon and burn out, that gifted people are eccentric, that they are physical weaklings, that there’s a thin line between genius and madness, that genius runs in families, that the gifted are so clever they don’t need special help, that giftedness is the same as having a high IQ, that some races are more intelligent or musical or mathematical than others, that genius goes unrecognised and unrewarded, that adversity makes men wise or that people with gifts have a responsibility to use them. Language has been enriched with such terms as ‘highbrow’, ‘egghead’, ‘blue-stocking’, ‘wiseacre’, ‘know-all’, ‘boffin’ and, for many, ‘intellectual’ is a term of denigration.

The nineteenth century saw considerable interest in the nature of genius, and produced not a few studies of famous prodigies. Perhaps for us today, two of the most significant aspects of most of these studies of genius are the frequency with which early encouragement and teaching by parents and tutors had beneficial effects on the intellectual, artistic or musical development of the children but caused great difficulties of adjustment later in their lives, and the frequency with which abilities went unrecognised by teachers and schools. However, the difficulty with the evidence produced by these studies, fascinating as they are in collecting together anecdotes and apparent similarities and exceptions, is that they are not what we would today call norm-referenced. In other words, when, for instance, information is collated about early illnesses, methods of upbringing, schooling, etc., we must also take into account information from other historical sources about how common or exceptional these were at the time. For instance, infant mortality was high and life expectancy much shorter than today, home tutoring was common in the families of the nobility and wealthy, bullying and corporal punishment were common at the best independent schools and, for the most part, the cases studied were members of the privileged classes. It was only with the growth of paediatrics and psychology in the twentieth century that studies could be carried out on a more objective, if still not always very scientific, basis.

Geniuses, however they are defined, are but the peaks which stand out through the mist of history and are visible to the particular observer from his or her particular vantage point. Change the observers and the vantage points, clear away some of the mist, and a different lot of peaks appear. Genius is a term we apply to those whom we recognise for their outstanding achievements and who stand near the end of the continuum of human abilities which reaches back through the mundane and mediocre to the incapable. There is still much truth in Dr Samuel Johnson’s observation, 'The true genius Is a mind of large general powers, accidentally determined to some particular direction’. We may disagree with the ‘general’, for we doubt if all musicians of genius could have become scientists of genius or vice versa, but there is no doubting the accidental determination which nurtured or triggered their gifts into those channels into which they have poured their powers so successfully. Along the continuum of abilities are hundreds of thousands of gifted men and women, boys and girls.

What we appreciate, enjoy or marvel at in thè works of genius or the achievements of prodigies are the manifestations of skills or abilities which are similar to, but so much superior to, our own. But that their minds are not different from our own is demonstrated by the fact that the hard-won discoveries of scientists like Kepler or Einstein become the commonplace knowledge of schoolchildren and the once outrageous shapes and colours of an artist like Paul Klee so soon appear on the fabrics we wear. This does not minimise the supremacy of their achievements, which outstrip our own as the sub-four-minute milers outstrip our jogging.

To think of geniuses and the gifted as having uniquely different brains is only reasonable If we accept that each human brain is uniquely different. The purpose of instruction is to make the US even more different from one another, and in the process of being educated we can learn from the achievements of those more gifted than ourselves. But before we try to emulate geniuses or encourage our children to do so we should note that some of the things we learn from them may prove unpalatable. We may envy their achievements and fame, but we should also recognise the price they may have paid in terms of perseverance, single-mindedness, dedication, restrictions on their personal lives, the demands upon their energies and time, and how often they had to display great courage to preserve their integrity or to make their way to the top.

Genius and giftedness are relative descriptive terms of no real substance. We may, at best, give them some precision by defining them and placing them in a context but, whatever we do, we should never delude ourselves into believing that gifted children or geniuses are different from the rest of humanity, save in the degree to which they have developed the performance of their abilities.</detail>


<details> <summary>Stop Trying to Be Indispensable at Work </summary>
“Be indispensable” is flawed advice. Yes, it may provide some job security but good luck trying to advance in your career.
![image.png](http://tva1.sinaimg.cn/large/008gLLJzgy1h7g26mediqj30pt0i079n.jpg)
You don’t have to do it all.


“Be indispensable.” This commonly given career advice is hard to argue with. No doubt it sounds appealing these days, with a softening economy and layoffs once again dominating the business news. It’s advice I’ve tried to follow for much of my own working life.


But the idea is deeply flawed.


Yes, being indispensable ought to be insurance against getting fired, and getting fired is horrible. Anyone who has been through even a single round of layoffs knows the anxiety it causes, the “Hunger Games”-ish feeling of needing to out-compete one’s friends and colleagues. Indispensability seems like the best armor — but that armor can become a cage.


Sometimes an effort to be indispensable turns an employee into a one-person bottleneck. But if they’re the single point of failure for a project, or the only person who knows how the system works, or the one employee the client is willing to talk to, it can be near impossible for them to leave — whether that’s taking time off for vacation or advancing to a bigger job. 


A boss might reluctantly think, “Janice has really earned a promotion, but we’d need to hire two people to replace her,” or “It’s not fair to keep sending Paul to deal with the angriest customers, but he’s the only one who can talk them down.”


These employees are so valuable in their current jobs that promoting them would create an immediate problem for their managers. In a 2020 survey by LinkedIn, talent professionals said the biggest barrier to internal recruiting was bosses wanting to hold on to their best people. This might not be a problem if those employees are happy, but because so many organizations require a promotion in order to get a raise, indispensable people can quickly become underpaid.


Say Janice were given the opportunity to move into management — maybe she could teach more employees to be as productive as she is, maybe she’d create 12 mini-Janices. With Paul’s hard-won knowledge of customer needs, perhaps he’d be better off in a new role, helping to redesign the product so it doesn’t make people so angry.


But their indispensability — and management’s shortsightedness — keeps them and the whole organization on the hamster wheel. This failure to promote people out of individual roles to that first rung of management is sometimes called a sticky floor, and it especially hurts women and people of color, stunting their careers before they ever have a chance to run into a glass ceiling.


In organizations that lean too heavily on indispensability, valuable people become incentivized to leave to get recognition. A 2022 McKinsey study found that more than 80% of role moves involved someone switching employers; internal promotions remain rare. Most job changes they studied also involved people taking on substantially new skills, indicating that they had abilities their old employer had overlooked. 


When these crucial employees eventually depart, sometimes that’s the first time bosses and colleagues realize the true extent of all the work they handled. It can take months for the team to recover. A paper by MIT economist Simon Jäger and IAB economist Jörg Heining (called “How Substitutable Are Workers? Evidence from Worker Deaths”) concluded that when departed workers can be readily replaced, wages rise as their colleagues quickly pick up the slack. When workers are harder to replace, it has a negative effect that hurts wages for those left behind.


Instead of aspiring to be indispensable, workers and companies should recognize that it’s better for everyone if employees are skilled and valuable but interchangeable. This would mean an employee can leave for vacation with minimal fuss, knowing that her colleagues will handle anything that comes up while she’s out. When workers can easily substitute for each other, their hours can be more predictable and also more flexible.


Interchangeability can even help close the wage gap between men and women: A study of pharmacists by Harvard economists Claudia Goldin and Lawrence Katz found that increased substitutability among pharmacists — thanks to uniform training, computerized customer data and the rise of national chains — has brought the profession some of the highest hourly pay and one of the lowest gender-pay gaps. 


In an organization with skilled, substitutable employees, managers can redistribute work seamlessly when they realize that one person has taken on too much, or that another doesn't have enough to do. It’s not a disaster if someone retires or quits. Customers aren’t upset if they don’t get to talk to a particular person. And when employees fill in for each other, fraud has fewer places to hide; that’s one reason some big banks make vacations mandatory.


If indispensability is so costly, why do we valorize it? Emotionally, it feels gratifying to be needed. And being indispensable might seem like good insurance against getting laid off. But as we’ve seen, that career insurance comes at a high premium. Nor is it foolproof — consider the layoffs rippling through top consulting and law firms over the past few weeks. Did any of those recently fired professionals consider themselves substitutable when they were pulling yet another 80-hour week? More likely, they thought their efforts were uniquely essential. 


Fungibility gets a bad rap. We associate it with being lower-value, with being a cog in a larger machine. But there are a lot of highly skilled, highly paid jobs where workers can step into one another’s shoes: pilots, tax accountants, software developers. Health care is full of examples. If you need surgery, do you forgo pain medication until your preferred anesthesiologist is available?


In most sectors, indispensability is a habit, not a law of physics. Take editing, a profession I’ve been in for almost 20 years. I’ve worked at places where editors substitute for one another, filling in when one is out on vacation, and at places where editors do not. Although it somewhat hurts my ego to admit it, the copy turns out just as well when we stand in for each other.


Indispensability isn’t only costly, it’s unnecessary. There are many other ways to impress one’s boss. Be diligent, meticulous, efficient, respectful, cheerful. Exceed expectations. Minimize errors. Take initiative. Figure out what your boss’s priorities are and make them your own. And let that be enough. ■
BySarah Green Carmichael</detail>



<details> <summary>apes, not angels </summary>
If we want to change our consumerist society, we need greener status signals that appeal to our animal instincts, says Solitaire Townsend.

TAYLOR SWIFT often tops the charts, but her most recent number one has drawn more censure than acclaim. Because, at 8293 tonnes of carbon emitted so far this year, she has been judged the world’s most polluting celebrity in terms of private jet use – easily beating Kylie Jenner, who infamously took a 17-minute flight in hers.

For many of my fellow environmentalists, this carbon-spurting excess reinforces disgust at our celebrity-obsessed, conspicuous-consuming culture. The only thing that will save the world from a greed-triggered apocalypse? Sweeping it all away.

This is a tempting idea, but it has one major snag: when  when environmentalists exhort us to care less about buying cars and clothes, and instead base our lifestyle choices on the planet' s limited resources, they ignore an elemental part of our hominid nature: preoccupation with status.

Status is, of course, the key that unlocks every animal's ultimate goal - breeding rights. Charles Darwin named the status displays rife in nature "honest signalling", noting that male peacocks' ostentatious feathers prove to females that they are so adept at finding food and avoiding predators they can afford to grow big, bright, utterly wasteful plumage. Some deer grow antlers so unwieldy that they become caught in trees for days. Other animals buy jewellery and fast cars. From the forest to the penthouse, all status signals say: "I am successful enough to have more than I need. See my superior genes!"

If wasteful excess is entirely natural, it poses a head-scratcher for environmentalism. We eco-activists emphasise that humans aren't above nature. But we can't have it both ways. If people are no better than any other mammal, how can we banish our natural instincts for status?

This paradox needs untangling, and fast, because massive social and behavioural change has moved centre stage in our struggle to meet the Paris climate agreement.

According to a report by the Intergovernmental Panel on Climate Change, sociocultural change could rapidly save 5 per cent of demand-side carbon. But I fear this gigatonne-level prize could remain unclaimed unless we embrace the realities of being great apes and forget exhortations to act like angels.

Rather than trying to conquer our evolved desire for status, we must switch the symbols used to display it. Luckily, status semiotics are malleable. Throughout history, humans have been wildly creative with ours, from foot-binding in late imperial China to poorer Victorians blackening their teeth with boot polish as only the rich could afford enamel-rotting sugar.

Society is ripe for sustainable status signals. According to a recent eBay report, 52 per cent of UK citizens feel guilty when buying unsustainable products - rising to 71 per cent among young people. And we are all more likely to purchase greener products when we are watched. Perhaps upcycled or rented clothes could carry a distinctive tag that makes sustainability a status display.Maybe travelling by train could earn covetable passport stamps. 

That said, it is in virtual worlds where status symbols have the greatest potential to radically cut material consumption. Your online status and virtual possessions could become more important than anything you own in the real world. That might not sound appealing, but status displays must remain anxiety-inducing, hierarchical and unfair - because that is the point of them.

By shifting from high-carbon, disposable, heavily material symbols to virtual, reusable and sustainable ones, we might buy more time to solve climate change. And if a bunch of great apes can decouple social status from its material impacts, we might just have the effect of angels.
</detail>


<details> <summary> Mental Illness and Dementia </summary>
Why do psychiatric conditions __multiply__ the risk of cognitive decline?

Age is the single biggest risk factor for dementia, with the odds doubling about every five years after age 65. But many things influence those odds for a __given individual__. Genetic vulnerability is a contributor, as are so-called modifiable risk factors such as smoking, cardiovascular disease, social isolation, and impaired hearing and vision. Certain mental conditions, particularly depression and schizophrenia, have also been linked to dementia. But because depression can itself be a sign of cognitive decline, the causality has been a bit muddy. Earlier this year an analysis of data from New Zealand provided the most convincing evidence to date linking many kinds of mental illness with dementia. That study raises important questions about the reasons for this increased risk and what could be done to reduce it.

The study looked at the health records of 1.7 million New Zea-landers born between 1928 and 1967 covering a 30-year period ending in mid-2018. It found that those with a diagnosed mental disorder--such as anxiety disorders, depression or bipolar disorder--had four times the rate of ultimately developing dementia compared with people without such a diagnosis. For those with a psychosis such as schizophrenia, it was six times the rate.
Among people who developed dementia, those with a psychiatric disorder were affected 5.6 years earlier, on average.

The study did not examine biological, social or other reasons for the increased risk, but research on dementia points to several possible explanations. "There might be shared genetic risk factors," suggests psychologist Leah Richmond-Rakerd of the University of Michigan, lead author of the study. Recent studies have found some overlap in genetic markers associated with Alzheimer's disease and those linked to bipolar disorder and to major depression. Long-term use of psychiatric medications could also be playing a role in dementia, but Richmond-Rakerd and her coauthors do not think it is a major contributor.

They suspect that a more significant risk factor is the __chronic__ stress associated with having a psychiatric disorder, which may __degrade__ brain health over time. Studies in animals as well as human autopsy studies have linked chronic stress to a loss of neural connections in the hippocampus, the brain's memory center, which is where Alzheimer's takes a heavy toll. Evidence suggests that stress drives __inflammation__ and __immune dysregulation__ in the body and brain, impacting brain connectivity, says Harvard University neurologist and dementia researcher Steven Arnold. _"If you have fewer connections and synapses to begin with because of stress, then you can't afford to lose as many with aging before it starts to show up as what we might call dementia."_ In other words, people with mental illnesses may have less "__cognitive reserve__"--brainpower that is sufficiently robust to withstand normal aging without obvious losses of function.

Vulnerability in this population may also be linked to their finding it more difficult to lead healthy lives, physically and social-ly, Richmond-Rakerd says. "They might exercise less, or drink alcohol excessively, or have trouble staying socially connected"-all of which increase the risk for dementia. People with certain psychiatric conditions tend to have higher-than-average rates of smoking and fewer years of education, which are also risk factors.

Could a more holistic approach to treating mental illness mitigate the risk for dementia? Researchers tend to think so. In 2020 the British-based Lancet Commission on dementia prevention estimated that four in 10 cases could be prevented or delayed if society did a better job of addressing 12 modifiable risk factors, including such psychosocial contributors as depression, poor social support and low education level. Progress on some of these factors may explain why dementia rates have already fallen 15 percent per decade for the past 30 vears in high-income countries. "We think there are two main reasons: better cardiovascular risk factor control and a big increase in education level, says Kenneth Langa of the University of Michigan, associate director of the Health and Retirement Study, one of the major efforts tracking these trends.

In an ideal world, Langa and other researchers say, efforts to prevent dementia would begin in childhood with strong investments in education and the inculcation of healthy habits. Such efforts would be incorporated into the treatment of depression and other mental illnesses that often emerge in the teen and early adult years. Sadly, we do not live in that ideal world; mental illness continues to be stigmatized and undertreated. But given the high costs to society and the personal tolls exacted by both mental illness and dementia, it's hard to imagine a wiser investment.
</detail>


<details> <summary>Seeing red</summary>
We have projected our hopes and fears onto Mars through the ages.
Even today, the Red Planet still spells adventure, says Stuart Clark

HERE is something about Mars that captivates us. Throughout history, different cultures and individuals have looked at the blood-red beacon in the night sky and filled it with whatever their imaginations associated with the unknown. Across time, Mars has been the celestial embodiment of warrior gods, an astrological talisman of spiritual influence, a venue for utopia and the wellspring of horrors beyond belief. Even today, our relationship with the Red Planet continues to evolve.
Ofthe five planets that are visible to the naked eye,
Mars is easily the most dramatic, and its observable behaviours have informed our cultural interpretations of it. Yet from the flighty apparition of Mercury in the twilight to the extraordinary beauty of Venus in the gathering evening, the unflinching progress of Jupiter and the laboured march of yellowed Saturn, Mars has stiff competition. Still, it stands out for two reasons
The first is its baleful red colour.
The second is that it is the only planet that appears to wax and wane significantly in brightness.
Sometimes it is an unmistakeable beacon, strangely daunting and burning brightly. At other times, it is so faint that it fades to the point ofobscurity against the stars.
Perhaps it is these comings and goings that have piqued our curiosity through the ages, inviting us to speculate. In his

一二:
book The Structure and Dynamics of the Psyche, published in 1960, noted psychiatrist Carl Jung wrote:
"The starry vault of heaven is in truth the open book of cosmic projection." He believed that when we look at celestial objects, we can't help but project our innate hopes and fears onto them.
Indeed, Mars has proven itself to be the perfect venue for such projection, as the vast library of stories set on the Red Planet can attest. Yet these works aren't wholly flights of fancy. The imaginations of writers like H. G. Wells, Ray Bradbury and Arthur C. Clarke were fed by the leading scientific

一二:
investigations of their age.
The late-19th century was a particularly fertile time for interest in Mars. Telescopes were just starting to resolve the planet's surface features, but, due to a strange quirk of human cognition, our brains joined these half-glimpsed landmarks into straight lines. This led to the idea that Mars was a dying planet, with its supposed inhabitants digging canals in a desperate attempt to irrigate the deserts.
So widespread was the belief in Martian life that, in 1891, when Anne Goguet established a prize of 100,000 francs in the name of her

一二:
son, Pierre Guzman, to be awarded to the first person to communicate with extraterrestrials, Martians were excluded from consideration on the grounds that talking to them would be too easy. Despite the canals being thoroughly debunked in the early decades of the 2oth century, they continued to be a motif in fiction until the second half of the century.
These days, we continue to shroud the planet in our hopes and fears. For evidence, look no further than the discussions around Elon Musk's aim to colonise Mars. And, while a view of the planet is lost to so many of us because we live in light-polluted areas, our mental image of Mars is perhaps more vivid than at any other time in history. Thanks to space exploration and the internet, many of us can be on Mars at the click of a mouse.
But why has Mars endured above all other planets in the public consciousness? I believe it is because the more we discover about it. the better it becomes.
It is a world composed of larger-than-life versions of Earth's most exciting landscapes. Mars has taller volcanoes, deeper canyons, colder deserts and larger dust storms. Everything about it spells adventure -and tha

一二:
and that is irresistible tous.</details>



<details> <summary>Our cult of perfection has devalued beauty</summary>

_The ugly truth is that originality in art and design has given way to a bland global uniformity_

Unmoved and slightly bored by the National Gallery's new exhibition of paintings by Raphael, I decide that I have surfeited on perfection. Confronting each harmonious arrangement of babies, fabric and flawless Italian sky, I find my eyes sliding hopefully towards the gift shop. The problem, obviously, is me, not Raphael.

To the people who first loved these pictures in the 16th century each one must have seemed a small zone of perfect sublimity in a chaotic, disfigured world. Today we are accustomed to the perfect. Or a particular, limited idea of the perfect.

To be alive (and connected to the internet) in the 21st century is to have seen more flawlessly beautiful people, sunsets and breakfasts than any human being who has previously lived. Perfection has become commonplace. Faces that would have once inspired local wars are now restricted to powering only moderately successful careers on Instagram.

The twin hallmarks of the modern cult of perfection are blandness and repetition. Search for sunsets on Instagram and you will discover thousands of them - identical reds and pinks burning away in neat rows as if somewhere a central committee issues directives governing the colours and compositions appropriate for photographs of the sky.

Though the internet has tended to divide us politically, its aesthetic effect is homogenising. "Instagram face" describes the ubiquitous ideal of modern beauty characterised by "high, plump cheekbones, full lips and catlike eyes". There are YouTube videos that provide exhaustive instruction in the use of face-tuning apps and makeup to make you look less like yourself and more like everybody else.

Blandness and repetition. Since the invention of the iPhone we have been subjected to the triumphant progress of a kind of infantile minimalism: bright colours; bold, obvious shapes; simple, instantly local eccentricities are shorn away and the world converges legible designs; a horror of ornament or elaboration. This is an aesthetic that offers the same uncomplicated reassurance as the set of a children's TV show. I spot its symptoms everywhere in London: vases in the shape of breasts and buttocks, cafes painted in pastel colours serving bright Instagram-friendly food, the simple blocky covers of (otherwise excellent) novels by Noise Dolan, Meg Mason and Sally Rooney.

These aesthetic tendencies did not begin with social media. In the late 20th century bland corporate minimalism emerged as the imperial style of global capitalism. As Corinthian columns once supported Roman buildings the breadth of the empire from Jordan to Hadrian's Wall, so today identical towers of steel and glass march across the citadels of finance from New York to London to Singapore.

In every one of these cities you can go to bed in a chicly spartan hotel room and rise to a morning flat white in a tasteful café surrounded by glossy-leaved pot plants. In every one of these cities, advertisements present you with ideal images of uncluttered, tastefully upholstered lives that might be lived anywhere at all.

Just as the great civic buildings of Rome were the incarnation of Roman power, so 21st-century minimalism embodies the values of international commerce. Because commerce depends on advertising, modern design values the direct, infantilising, eye-catching language of adverts.

Hence the bum-shaped vases, bold colours and unadorned book-covers.
Hence the London skyscrapers built ir big, babyish shapes - the Shard, the Cheesegrater, the Walkie-Talkie, the Gherkin - which aspire all too desperately to "iconic" status. And as the highest value of commerce is efficiency and value for money, so infantile minimalism is ostentatiously suspicious of waste, eschewing ornament for clean monotonous lines. We must make do instead with the bland but cost-effective consolations of colour and form.

Most of all. infantile minimalism is a style that must make sense anywhere. Norman Foster's profits are healthiest when he can build his silvery leviathans in any city in the world. Apple's iPhones must fit in as easily in Tokyo as California. Every Instagram breakfast must garner

一二:
likes as efficiently among followers in Europe as in America. And so local eccentricities are shorn away and the world converges on a narrow set of idealised, endlessly repeatable forms.

When the mayor of London, Sadig Khan, described the newly opened Crossrail stations - those barren monochrome concourses - as modern "cathedrals" he was right, but the thought is depressing, not inspiring. When I was a child, the old Tottenham Court Road station with its escalators descending through
Eduardo Paolozzis mosaiced arches seemed to me like a magic cavern. It was unique to London. It only made sense there. The new Tottenham Court Road station, built for Crossrail, would not seem out of place in China or Singapore or America or anywhere at all. Nobody could find it really ugly or objectionable.

Our world is more tasteful than ever, an effect the internet has accelerated. The safest speech online is completely inoffensive. So is the most successful design. But art thrives on local oddness, eccentricity and even offence. When WH Auden wrote that it was his "hope to be, like some valley cheese, local, but prized elsewhere" he spoke for all artists.

Beautiful things can be odd and uneven. 'They spring from local particularity and are not pre-engineered for the appreciation of a global audience.
Too late. We are surrounded by a bland monotonous perfection that has impoverished our idea of beauty.
And so I find myself standing in front of a painting by Raphael and yawning.</details>


[Protests against strict COVID-zero policy are sweeping China.](https://theconversation.com/protests-against-strict-covid-zero-policy-are-sweeping-china-its-anyones-guess-what-happens-now-195442)

中国的 COVID 措施是世界上最严格的措施之一，因为它继续寻求封锁以抑制病毒

毫无疑问，变革的压力是存在的，尽管很难预测如何实现这一目标。
